{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "plt.rcParams.update({\"font.size\": 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Proccess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv(\n",
    "    \"/Users/vigrel/Git/NLP/nlp-intermediate-exam/data/WELFake_Dataset.csv\"\n",
    ")\n",
    "original = original.dropna(subset=\"text\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return \" \".join(\n",
    "        [\n",
    "            lemmatizer.lemmatize(word)\n",
    "            for word in text.split()\n",
    "            if word.lower() not in stop_words\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "original[\"text\"] = original.text.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original.copy()\n",
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vectorizer = CountVectorizer(\n",
    "    binary=True,\n",
    "    lowercase=True,\n",
    ")\n",
    "\n",
    "full_X = full_vectorizer.fit_transform(data.text)\n",
    "full_word_doc_freq = np.asarray(full_X.sum(axis=0)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    binary=True,\n",
    "    min_df=0.05,\n",
    "    max_df=0.8,\n",
    "    lowercase=True,\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(data.text)\n",
    "word_doc_freq = np.asarray(X.sum(axis=0)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(full_word_doc_freq, bins=30, log=True, alpha=0.5, color=\"orange\", label=\"full\")\n",
    "plt.hist(word_doc_freq, bins=30, log=True, alpha=0.5, color=\"blue\", label=\"partial\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Document Frequency of Words\")\n",
    "plt.ylabel(\"Count of Words\")\n",
    "plt.title(\"Word Document Frequency Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[\"text\"], data[\"label\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", vectorizer),\n",
    "        (\"model\", LogisticRegression()),\n",
    "    ]\n",
    ")\n",
    "model = pipe.fit(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "sns.heatmap(\n",
    "    confusion_matrix(y_test, pred),\n",
    "    annot=True,\n",
    "    fmt=\"\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"R-News\", \"F-News\"],\n",
    "    yticklabels=[\"R-News\", \"F-News\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"Real Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pipe[\"model\"].coef_\n",
    "classes = pipe[\"model\"].classes_\n",
    "vocabulary = pipe[\"vectorizer\"].vocabulary_\n",
    "\n",
    "words_and_weights = [\n",
    "    (coefs[0, idx], word)\n",
    "    for word, idx in vocabulary.items()\n",
    "    if not word.isnumeric() and len(word) > 3\n",
    "]\n",
    "\n",
    "sorted_tuples = sorted(words_and_weights)\n",
    "counts, words = zip(*sorted_tuples)\n",
    "\n",
    "num_words = 10\n",
    "x_axis = np.arange(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\n",
    "    \"Top Features for Reliable News\",\n",
    ")\n",
    "bars = plt.bar(\n",
    "    x_axis[0:num_words],\n",
    "    [abs(c) for c in counts[0:num_words]],\n",
    "    color=\"green\",\n",
    "    width=0.6,\n",
    "    alpha=0.75,\n",
    ")\n",
    "plt.xticks(x_axis[0:num_words], words[0:num_words], rotation=70, ha=\"right\")\n",
    "plt.ylabel(\"Coefficient Weight\")\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        yval,\n",
    "        -round(yval, 2),\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "plt.gca().set_yticks([])\n",
    "plt.gca().spines[\"top\"].set_visible(False)\n",
    "plt.gca().spines[\"right\"].set_visible(False)\n",
    "plt.gca().spines[\"left\"].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Top Features for Fake News\")\n",
    "bars = plt.bar(\n",
    "    x_axis[-num_words:], counts[-num_words:], color=\"red\", width=0.6, alpha=0.6\n",
    ")\n",
    "plt.xticks(x_axis[-num_words:], words[-num_words:], rotation=70, ha=\"right\")\n",
    "plt.ylabel(\"Coefficient Weight\")\n",
    "\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        yval,\n",
    "        round(yval, 2),\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "plt.gca().set_yticks([])\n",
    "plt.gca().spines[\"top\"].set_visible(False)\n",
    "plt.gca().spines[\"right\"].set_visible(False)\n",
    "plt.gca().spines[\"left\"].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(data, vectorizer):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data[\"text\"],\n",
    "        data[\"label\"],\n",
    "        test_size=0.2,\n",
    "        random_state=None,\n",
    "        stratify=data[\"label\"],\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"vectorizer\", vectorizer),\n",
    "            (\"model\", LogisticRegression()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model = pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return precision, recall, f1, balanced_accuracy\n",
    "\n",
    "\n",
    "num_iterations = 100\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(train_and_evaluate)(data, vectorizer) for _ in range(num_iterations)\n",
    ")\n",
    "\n",
    "precision_scores, recall_scores, f1_scores, balanced_accuracy_scores = zip(*results)\n",
    "\n",
    "mean_precision = np.mean(precision_scores)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_balanced_accuracy = np.mean(balanced_accuracy_scores)\n",
    "\n",
    "print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "print(f\"Mean F1 Score: {mean_f1:.4f}\")\n",
    "print(f\"Mean Balanced Accuracy: {mean_balanced_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_list = np.logspace(0, -4, 25)\n",
    "\n",
    "\n",
    "def get_train_test_accuracy(plot_list, genre_list):\n",
    "    model_lr = pipe\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        plot_list, genre_list, test_size=0.2, random_state=None\n",
    "    )\n",
    "    model_lr.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model_lr.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "    y_test_pred = model_lr.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "\n",
    "def process_fraction(f, data):\n",
    "    accuracy_f_train = []\n",
    "    accuracy_f_test = []\n",
    "    for _ in range(50):\n",
    "        df_sample = data.sample(frac=f)\n",
    "        train_accuracy, test_accuracy = get_train_test_accuracy(\n",
    "            df_sample[\"text\"], df_sample[\"label\"]\n",
    "        )\n",
    "\n",
    "        accuracy_f_train.append(train_accuracy)\n",
    "        accuracy_f_test.append(test_accuracy)\n",
    "\n",
    "    return np.mean(accuracy_f_train), np.mean(accuracy_f_test)\n",
    "\n",
    "\n",
    "def get_accuracy_sample(df, f_list):\n",
    "    results = Parallel(n_jobs=-1)(delayed(process_fraction)(f, df) for f in f_list)\n",
    "\n",
    "    train_accuracies, test_accuracies = zip(*results)\n",
    "\n",
    "    return train_accuracies, test_accuracies\n",
    "\n",
    "\n",
    "train_accuracies, test_accuracies = get_accuracy_sample(data, f_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "error_train = [1 - x for x in train_accuracies]\n",
    "error_test = [1 - x for x in test_accuracies]\n",
    "\n",
    "plt.plot(f_list * len(data), error_test, label=\"Test\")\n",
    "plt.plot(f_list * len(data), error_train, label=\"Train\")\n",
    "\n",
    "# plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines[\"right\"].set_visible(False)\n",
    "plt.gca().spines[\"left\"].set_visible(False)\n",
    "plt.ylim(ymax=0.15, ymin=0)\n",
    "\n",
    "plt.gca().set_yticks([0, 0.05, 0.1, 0.15])\n",
    "plt.title(\"Training curves\")\n",
    "plt.ylabel(\"Error rate\\n(1 - accuracy)\")\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.axhline(\n",
    "    y=(error_test[0] + error_train[0]) / 2, color=\"gray\", linestyle=\"--\", alpha=0.3\n",
    ")\n",
    "plt.text(\n",
    "    0,\n",
    "    0.068,\n",
    "    round((error_test[0] + error_train[0]) / 2, 3),\n",
    "    ha=\"center\",\n",
    "    va=\"bottom\",\n",
    "    color=\"gray\",\n",
    "    alpha=0.3,\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=2)\n",
    "X_nmf = nmf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 20\n",
    "\n",
    "\n",
    "def print_words_in_topics(nmf, vectorizer):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    for idx, topic in enumerate(nmf.components_):\n",
    "        print(f\"Topic {idx}\")\n",
    "        for i in topic.argsort()[-n_words:]:\n",
    "            print(words[i])\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_words_in_topics(nmf, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Topic 0: Generic News\n",
    "* Topic 1: Elections News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vigrel/Git/NLP/minicrawler/.venv/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for topic 0:\n",
      "Mean Precision: 0.9220\n",
      "Mean Recall: 0.9231\n",
      "Mean F1 Score: 0.9222\n",
      "Mean Balanced Accuracy: 0.8860\n",
      "\n",
      "Classification report for topic 1:\n",
      "Mean Precision: 0.9356\n",
      "Mean Recall: 0.9354\n",
      "Mean F1 Score: 0.9355\n",
      "Mean Balanced Accuracy: 0.9284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(topic_X, topic_y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        topic_X, topic_y, test_size=0.2, random_state=None\n",
    "    )\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return precision, recall, f1, balanced_accuracy\n",
    "\n",
    "\n",
    "topic_assignments = np.argmax(X_nmf, axis=1)\n",
    "\n",
    "for topic in range(nmf.n_components):\n",
    "    topic_indices = np.where(topic_assignments == topic)[0]\n",
    "    topic_X = X[topic_indices]\n",
    "    topic_y = data.label.iloc[topic_indices]\n",
    "\n",
    "    num_iterations = 100\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(train_and_evaluate)(topic_X, topic_y) for _ in range(num_iterations)\n",
    "    )\n",
    "\n",
    "    precision_scores, recall_scores, f1_scores, balanced_accuracy_scores = zip(*results)\n",
    "\n",
    "    mean_precision = np.mean(precision_scores)\n",
    "    mean_recall = np.mean(recall_scores)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    mean_balanced_accuracy = np.mean(balanced_accuracy_scores)\n",
    "\n",
    "    print(f\"Classification report for topic {topic}:\")\n",
    "    print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "    print(f\"Mean F1 Score: {mean_f1:.4f}\")\n",
    "    print(f\"Mean Balanced Accuracy: {mean_balanced_accuracy:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
